---
title: "Using `unihtee`"
author: "[Philippe Boileau](https://pboileau.ca)"
date: "`r Sys.Date()`"
bibliography: ../inst/references.bib
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{using-unihtee}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE}
library(dplyr)
library(sl3)
library(Rsolnp)
library(unihtee)
```

# Background

Treatment effect modifiers (TEMs) are responsible for the disparate effects of a
treatment on a population. They are confounders --- meaning they influence both
the likelihood of receiving treatment and the outcome --- that modify the effect
of treatment on the outcome. In precision medicine, these effect modifiers
delineate patient subgroups which experience differing benefit from a given
medical intervention. In public health, they are used to determine the effect of
policy decisions on sub-populations. Identifying TEMs, if any exist, is required
for a comprehensive understanding of an intervention's effect on a population.

Traditional parametric modeling techniques, like generalized linear models
(GLMs), define TEMs as the confounders with non-zero confounder-treatment
interaction terms. In settings characterized by time-to-event outcomes, the Cox
proportional hazards model can similarly be used. Inference about TEMs is
therefore possible under stringent conditions about the data-generating process.
When these (semi)parametric methods' assumptions, like linearity of the
confounder-outcome relationship in a linear model or proportional hazards in a
Cox model, are violated, however, their results are invalid.

More flexible approaches instead focus on estimating the conditional average
treatment effect (CATE) using interpretable modeling techniques, like the LASSO
[@tibshirani1996; @tian2014; @chen2017; @zhao2018; @semenova2021;
@bahamyirou2022] or Random Forests [@breiman2001; @wager2018; @cui2022]. Again,
however, these methods require restrictive assumptions about the data-generating
process, like sparsity of treatment effect modification [@zhao2006],
approximately uncorrelated confounders [@zhao2006; @esl2009] and sample sizes
that are much larger than the number of confounders [@zhao2006], to reliably
recover TEMs.

Instead of identifying TEMs indirectly by modeling the
confounder-treatment-outcome relationship, recent work has developed frameworks
tailored to the task [@williamson2022framework; @boileau2022; @hines2022TEVIP;
@boileau2023]. Relying on TEM variable importance parameters (TEM-VIP), it is
possible to assess confounders' influence on the treatment effect in
nonparametric, algorithm-agnostic fashion that largely avoids the pitfalls of
parametric and CATE-based methodologies. These TEM-VIPs are defined within
nonparametric statistical models that may be augmented with causal
interpretations, permitting formal inference about TEMs.

The `unihtee` R package implements the recent proposals of @boileau2022 and
@boileau2023. This TEM-VIP framework relies on parameters assess the marginal
effect of each confounder on treatment effect heterogeneity, however treatment
effect is defined. Nonparametric estimators are provided in these works, and
their asymptotic properties established. In particular, these estimators are
shown to be asymptotically linear under minimal assumptions about the
data-generating process, and to recover treatment effect modifiers more readily
than competing methods. These TEM-VIPs and estimators are introduced alongside
worked examples in the following section.

# `unihtee` in Action

The `unihtee` R package performs inference about TEM-VIPs in data-generating
processes with a binary exposure variable and either continuous, binary, or
right-censored time-to-event outcomes. No restriction is placed on the
confounding variables, though ordinal confounders should be formatted as factors
using `ordered()`, and categorical confounders should be one-hot encoded.

## Working with Continuous Outcomes

Let there be $n$ independent and identically distributed (i.i.d.) random vectors
${X_i}_{i=1}^n$ such that $X_i = (W_i, A_i, Y^{(0)}, Y_i^{(1)}) \sim P_{X,0} \in
\mathcal{M}_X$. Dropping the index where possible for notational convenience for
the remainder of the tutorial, $W$ is defined as the set of $p$ confounders, $A$
as the binary treatment indicator, and $Y^{(0)}, Y_i^{(1)}$ the continuous
potential outcomes [@rubin1974] produced under control and treatment conditions,
respectively. Further, $\mathcal{M}_X$ is the full-data nonparametric model of
all possible data-generating processes, and contains the true but unknown
full-data data-generating process $P_{X,0}$.

Now, $P_{X,0}$ is generally unknown because its random vectors are unobservable:
only one potential outcome is ever measurable in practice. Still, $P_{X,0}$
allows to us to readily define causal parameters on which inference may
subsequently be performed. Examples are given in the subsections below.

We instead have access to $n$ i.i.d. random observations $O=(W,A,Y) \sim P_0 \in
\mathcal{M}$, where $W$ and $A$ are defined as in the full-data, and where $Y =
AY^{(1)} + (1-A)Y{(0)}$. $\mathcal{M}$ is the observed-data statistical model
containing all possible observed-data data-generating processes, including the
true data-generating process $P_{0}$. Note too that the elements of
$\mathcal{M}$ are fully specified by their full-data counterparts in
$\mathcal{M}_{X}$.

Having established the requisite statistical objects, let's define a
data-generating process:
```{r cont-dgp}
cont_outcome_dgp <- function(n_obs) {

  # confounders
  w_1 <- rnorm(n = n_obs)
  w_2 <- rnorm(n = n_obs)
  w_3 <- rnorm(n = n_obs)
  w_4 <- rnorm(n = n_obs)
  w_5 <- rnorm(n = n_obs)

  # treatment
  prop_score <- plogis(w_1 + w_2)
  a <- rbinom(n_obs, 1, prob = prop_score)

  # potential outcomes
  y_1 <- rnorm(n = n_obs, mean = w_1 + 2 * w_2 + 2 * w_3 + 0.5 * a, sd = 0.1)
  y_0 <- rnorm(n = n_obs, mean = w_1 + w_2, sd = 0.1)

  # outcome
  y <- a * y_1 + (1 - a) * y_0

  # assemble the observations in a tibble
  tibble(
    w_1 = w_1,
    w_2 = w_2,
    w_3 = w_3,
    w_4 = w_4,
    w_5 = w_5,
    a = a,
    y = y
  )
}
```

It's clear from the definition of the potential outcomes that `w_2` and `w_3`
are treatment effect modifiers: they interact with the treatment indicator `a`.
A TEM-VIP for quantifying the strength of the treatment effect modification is
outlined next.

### Additive TEM-VIP

Indexing $W$ by $j=1,\ldots,p$, and assuming without
loss of generality that
$\mathbb{E}_{P_{X,0}}[W_j] = 0, \mathbb{E}_{P_{X,0}}[W_j^2] > 0$, we can define
our first TEM-VIP for the $j^\text{th}$ confounder as follows:

$$
\Psi^F_j(P_{X,0}) =
\frac{\mathbb{E}_{P_{X,0}}[(Y^{(1)}-Y^{(0)})W_j]}{\mathbb{E}_{P_{X,0}}[W_j^2]}
=\frac{\mathbb{E}_{P_{X,0}}[(\mathbb{E}_{P_{X,0}}[Y^{(1)}|W]-
  \mathbb{E}_{P_{X,0}}[Y^{(0)}|W])W_j]}
  {\mathbb{E}_{P_{X,0}}[W_j^2]} \;.
$$

This parameter is called the additive TEM-VIP. Assuming the expectation of
$\mathbb{E}_{P_{X,0}}[Y^{(1)}|W]- \mathbb{E}_{P_{X,0}}[Y^{(0)}|W]$ conditional
on $W_j$ is linear in $W_j$, $\Psi^F_j(P_{X,0})$ is the simple linear regression
coefficient produced by regressing the difference in expected potential outcomes
against confounder $W_j$. Even when this relationship is nonlinear, as is almost
surely the case in most applications, $\Psi^F_j(P_{X,0})$ corresponds to the
correlation between the difference in potential outcomes and the $j^\text{th}$
confounder, re-normalized to be on the same scale as the potential outcomes.

Of course, as stated earlier, we don't generally have access to the full-data
random vectors required to perform inference about the above parameter. Luckily,
under the identification conditions outlined in @boileau2022 and @boileau2023
--- namely, that there is no unmeasured confounding and no positivity violations
--- we can perform inference about the equivalent observed-data parameter:

$$
\Psi_j(P_0)
=\frac{\mathbb{E}_{P_0}[(\mathbb{E}_{P_{0}}[Y|A=1,W]-
  \mathbb{E}_{P_0}[Y|A=0,W])W_j]}{\mathbb{E}_{P_0}[W_j^2]} 
= \Psi^F_j(P_{X,0})\;.
$$

@boileau2022 and @boileau2023 derive two nonparametric estimators of this
parameter: the one-step and targeted maximum likelihood (TML) estimators. Both
require the estimation of two nuisance parameters: the propensity score and the
expected outcome conditioned on the treatment and confounders. These estimators
are double robust: only one nuisance parameter is required to be consistently
estimated to ensure that the one-step and TML estimators are consistent. If both
nuisance parameters converge to their true values at a fast enough rate, then
the one-step and TML estimators are asymptotically normal. This permits
hypothesis testing about $\Psi_j(P_0)$ using Wald-type confidence intervals.
Further details on these estimators and their asymptotic properties are provided
in @boileau2022 and @boileau2023.

Having defined the parameter and briefly discussed the estimators, we apply
`unicate()` to recover the treatment effect modifiers, as defined by the
additive TEM-VIP. Both the one-step and TML estimators are showcased. The LASSO
regression of @tibshirani1996 is used to estimate the nuisance parameters.

```{r additive-tem-vip}
set.seed(514)

# simulate a random sample
sample_df <- cont_outcome_dgp(n_obs = 500)

# one-step estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3", "w_4", "w_5"),
  modifiers =  c("w_1", "w_2", "w_3", "w_4", "w_5"),
  exposure = "a",
  outcome = "y",
  outcome_type = "continuous",
  scale = "absolute",
  estimator = "onestep",
  cond_outcome_estimator = sl3::Lrnr_glmnet$new(
    family = "gaussian",
    formula = "~ a * w_1 + a * w_2 + a * w_3 + a * w_4 + a * w_5"
  ),
  prop_score_estimator = sl3::Lrnr_glmnet$new(family = "binomial")
)

# TML estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3", "w_4", "w_5"),
  modifiers =  c("w_1", "w_2", "w_3", "w_4", "w_5"),
  exposure = "a",
  outcome = "y",
  outcome_type = "continuous",
  scale = "absolute",
  estimator = "tmle",
  cond_outcome_estimator = sl3::Lrnr_glmnet$new(
    family = "gaussian",
    formula = "~ a * w_1 + a * w_2 + a * w_3 + a * w_4 + a * w_5"
  ),
  prop_score_estimator = sl3::Lrnr_glmnet$new(family = "binomial")
)
```

`unicate()` outputs a table of results summarizing the TEM-VIP inference
procedure for each potential treatment effect modifier. This table ordered by
the nominal p-value of the inference procedure. Both the one-step and TML
estimators correctly identify the TEM-VIP of `w_2` and `w_3` as being non-zero
based on the false discovery rate adjusted p-values. Each estimator's point
estimates are also close to their true parameter values; for every unit increase
in `w_2`, the one-step and TML estimators suggest that the average treatment
effect increases by $\approx 0.88$ and $\approx 1.02$ units, respectively.
Similarly for `w_3`, the one-step and TML estimators indicate that a unit
increase in this confounder leads to an increase of $\approx 2.04$ and $\approx
1.94$ in the average treatment effect, respectively.


## Working with Binary Outcomes

In the binary outcome setting, the full-data and observed-data models and
data-generating processes are identical to those of the continuous outcome
setting, save that the outcome is binary. That means the additive TEM-VIP can be
used as is with binary-outcome data --- the only practical difference is the
choice of estimator used for the expected outcome conditional on the treatment
and confounders. With that said, a relative TEM-VIP is generally more sensitive
and informative in these scenarios. We propose such a parameter in the following
section.

Before that, however, we define a data-generating process with two treatment
effect modifiers:

```{r binary-dgp}
bin_outcome_dgp <- function(n_obs) {

  # confounders
  w_1 <- rpois(n = n_obs, lambda = 3)
  w_2 <- rnorm(n = n_obs)
  w_3 <- rbinom(n = n_obs, size = 1, prob = 0.5)

  # treatment
  prop_score <- plogis(-0.5 + 0.25 * w_1 + w_2 + 0.5 * w_3)
  a <- rbinom(n_obs, 1, prob = prop_score)

  # potential outcomes
  y_prob_1 <- plogis(-1 - 3 * w_3)
  y_prob_0 <- plogis(1 + 2 * w_2)
  y_1 <- rbinom(n_obs, 1, prob = y_prob_1)
  y_0 <- rbinom(n_obs, 1, prob = y_prob_0)

  # outcome
  y <- a * y_1 + (1 - a) * y_0

  # assemble the observations in a tibble
  tibble(
    w_1 = w_1,
    w_2 = w_2,
    w_3 = w_3,
    a = a,
    y = y
  )
}
```

Here, `w_2` and `w_3` are a treatment effect modifier. They interact with the
treatment variable, `a`.


### Relative TEM-VIP

Again assuming that the confounders are centered at zero with non-zero variance,
the relative TEM-VIP for the $j^\text{th}$ confounder is
$$
\Gamma^F_j(P_{X,0})
=\frac{\mathbb{E}_{P_{X,0}}[(\log\mathbb{E}_{P_{X,0}}[Y^{(1)}|W]-
  \log\mathbb{E}_{P_{X,0}}[Y^{(0)}|W])W_j]}
  {\mathbb{E}_{P_{X,0}}[W_j^2]} \;.
$$

Assuming that the expectation of the log ratio of the expected conditional
potential outcomes conditional on $W_j$ is linear in $W_j$,
$\Gamma^F_j(P_{X,0})$ is the simple linear regression coefficient obtained by
regressing the log ratio of the expected conditional potential outcomes on
$W_j$. As with $\Psi^F_j(P_{X,0})$, this parameter can be interpreted as
standardized correlation coefficient of the log ratio of expected potential
outcomes and the $j^\text{th}$ confounder.

We highlight that this TEM-VIP isn't restricted to data-generating processes
with binary outcomes; it may also be used when the outcome is non-negative,
continuous or discrete random variable.

Again, much like $\Psi^F_j(P_{X,0})$, $\Gamma^F_j(P_{X,0})$ isn't generally ever
observed since it is a parameter of the full-data model. Under the conditions of
no unmeasured confounding and in the absence of positivity violations, however,
an equivalent parameter can be estimated from the observed-data:

$$
\Gamma_j(P_0)
=\frac{\mathbb{E}_{P_0}[(\log\mathbb{E}_{P_{0}}[Y|A=1,W]-
  \log\mathbb{E}_{P_0}[Y|A=0,W])W_j]}{\mathbb{E}_{P_0}[W_j^2]} 
= \Gamma^F_j(P_{X,0})\;.
$$

Nonparametric one-step and TML estimators of this parameter are derived by
@boileau2023 and implemented in the `unicate` package. Just like $\Psi_j(P_0)$,
inference about this parameter depends on the estimation of the propensity score
and the expected outcome conditional on treatment indicator and confounders.
These estimators are consistent if both nuisance parameters are consistently
estimated, and are asymptotically normal if both converge to their true values
at rate of $n^{-1/4}$. The latter permits hypothesis testing based on the
Gaussian. Additional details are provided by @boileau2023.

We now apply these estimators to a random sample generated by
`bin_outcome_dgp()`. The nuisance parameters are estimated using Random Forests
implemented in `ranger` package [@breiman2001; @ranger]. Note too that these
estimators are fit using cross-fitting [@zheng2011cross; @cherno2017].

```{r relative-tem-vip}
# simulate a random sample
sample_df <- bin_outcome_dgp(n_obs = 500)

# one-step estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers =  c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "y",
  outcome_type = "binary",
  scale = "relative",
  estimator = "onestep",
  cond_outcome_estimator = sl3::Lrnr_ranger$new(),
  prop_score_estimator = sl3::Lrnr_ranger$new(),
  cross_fit = TRUE
)

# TML estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers =  c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "y",
  outcome_type = "binary",
  scale = "relative",
  estimator = "tmle",
  cond_outcome_estimator = sl3::Lrnr_ranger$new(),
  prop_score_estimator = sl3::Lrnr_ranger$new(),
  cross_fit = TRUE
)
```

Both estimators correctly identify `w_2` and `w_3` as treatment effect
modifiers, as defined by the relative TEM-VIP. Note that the TML estimators'
estimates of `w_2` and `w_3` is noticeably smaller than that produced by the
one-step estimator; @boileau2023 have shown in a simulation study of this
estimator that the TML estimator exhibits larger finite-sample bias than the
one-step estimator. We therefore recommend using the one-step estimator in
smaller samples. Hence, the one-step estimator's results suggest that for every
unit increase in `w_2` and `w_3`, the log ratio of the conditional expected
potential outcomes is expected to decrease by $\approx 1.06$ and $\approx 3.00$
units, respectively.

## Working with Time-to-Event Outcomes

`unihtee` also provides functionality for estimating TEM-VIPs for data with
right-censored time-to-event outcomes. We detail the assumed full-data and
observed-data models here, then introduce an additive and a relative TEM-VIP in
subsequent sections.

Consider $n$ i.i.d. random vectors $X = (W, A, C^{(0)}, C^{(1)}, T^{(0)},
T^{(1)}) \sim P_{X,0} \in \mathcal{M}_X$. $W, A, P_{X,0},$ and $\mathcal{M}_X$
are defined as in the continuous and binary outcome models. $C^{(0)}$ and
$C^{(1)}$ correspond to the censoring times that would be observed under the
control and treatment conditions, respectively. Similarly, $T^{(0)}$ and
$T^{(1)}$ are the event times that would be observed under either treatment
assignment.

Again, $P_{X,0}$ is generally unobservable. We instead have access to random
observations $O = (W, A, \Delta, T) \sim P_0 \in \mathcal{M}$, where $\Delta$ is
a censoring indicator and $T = A(\Delta C^{(1)} + (1-\Delta)T^{(1)}) +
(1-A)(\Delta C^{(0)} + (1-\Delta) T^{(0)})$. $W, A, P_0,$ and $\mathcal{M}$ are
defined as in the previous observed-data models.

Now, it's worth introducing the full-data and observed-data conditional survival
functions since both TEM-VIPs introduced in this section rely on them. The
full-data survival function is defined as $S_{P_{X,0}}(t|A,W) =
\mathbb{P}_{P_{X,0}}[T^{(A)} > t | W]$. This parameter's observed-data
counterpart is defined analogously as $S_0(t|A,W) = P_{P_0}[T > t |A, W]$.

Finally, we define a time-to-event data-generating process with nine equidistant
time points and three potential confounders, `w_1`, `w_2`, and `w_3`. Of these
confounders, only `w_1` is a treatment effect modifier. Note that `unihtee()`
requires a "wide" data format when working time-to-event outcome data. That is,
each observation is represented by a single row in the data. An example is
provided following this code chunk.

```{r tte-dgp}
# define hazard functions
cond_cens_hazard <- function(time, a, w_1, w_2, w_3) {
  (time < 9) / (1 + exp(4 + 0.2 * (w_1 + w_2) - a))
}
cond_surv_hazard <- function(time, a, w_1, w_2, w_3) {
  (time < 9) / (1 + exp(2 + 3 * a * w_1)) + (time == 9)
}

# failure time simulator
failure_time_sim <- function(n_obs, a, w_1, w_2, w_3) {
  sapply(
    seq_len(n_obs),
    function(obs) {
      failure_time <- NA
      for (t in 1:9) {
        prob <- cond_surv_hazard(t, a, w_1[obs], w_2[obs], w_3[obs])
        status <- rbinom(1, 1, prob)
        if (status == 1) {
          failure_time <- t
          break
        }
      }
      return(failure_time)
    }
  )
}

# censoring time simulator
censor_time_sim <- function(n_obs, a, w_1, w_2, w_3) {
  sapply(
    seq_len(n_obs),
    function(obs) {
      censor_time <- NA
      for (t in 1:9) {
        prob <- cond_cens_hazard(t, a, w_1[obs], w_2[obs], w_3[obs])
        status <- rbinom(1, 1, prob)
        if (status == 1) {
          censor_time <- t
          break
        }
      }
      if (is.na(censor_time)) censor_time <- 10
      return(censor_time)
    }
  )
}

# time-to-event outcome data-generating process
tte_outcome_dgp <- function(n_obs) {

  # confounders
  w_1 <- rnorm(n = n_obs)
  w_2 <- rnorm(n = n_obs)
  w_3 <- rnorm(n = n_obs)

  # treatment
  prop_score <- plogis(-1 - w_1 + w_2 - w_3)
  a <- rbinom(n_obs, 1, prob = prop_score)

  # generate the failure events for t = 1 to 9
  failure_time_1 <- failure_time_sim(n_obs, 1, w_1, w_2, w_3)
  failure_time_0 <- failure_time_sim(n_obs, 0, w_1, w_2, w_3)

  # generate the censoring events for t = 1 to 9
  censor_time_1 <- censor_time_sim(n_obs, 1, w_1, w_2, w_3)
  censor_time_0 <- censor_time_sim(n_obs, 0, w_1, w_2, w_3)

  # compile the failure and censoring times
  failure_time <- sapply(
    seq_len(n_obs),
    function(obs) {
      if (a[obs] == 1) failure_time_1[obs] else failure_time_0[obs]
    }
  )
  censor_time <- sapply(
    seq_len(n_obs),
    function(obs) {
      if (a[obs] == 1) censor_time_1[obs] else censor_time_0[obs]
    }
  )
  
  # determine the observed time-to-event and censoring indicator
  time <- sapply(
    seq_len(n_obs),
    function(obs) {
      if (censor_time[obs] < failure_time[obs]) {
        censor_time[obs]
      } else {
        failure_time[obs]
      }
    }
  )
  censoring_indicator <- sapply(
    seq_len(n_obs),
    function(obs) if (time[obs] == censor_time[obs]) 1 else 0
  )
  
  # assemble the tibble
  tibble(
    w_1 = w_1,
    w_2 = w_2,
    w_3 = w_3,
    a = a,
    time = time,
    censoring_indicator = censoring_indicator
  )
}
```

```{r tte-data-example}
tte_outcome_dgp(10)
```

### Additive TEM-VIP

@boileau2023 Propose an additive TEM-VIP based on the restricted mean survival
time at some pre-specified time $t$. Again assuming without loss of generality
that the $\mathbb{E}_{P_{X,0}}[W_j] = 0$ and that the confounders have variance
bounded away from zero, this TEM-VIP is defined as
$$
\Psi_j^F(P_{X,0};t) = \frac{\mathbb{E}_{P_{X,0}}\left[W_j
  \int_0^t \{ S_{P_{X,0}}(u|1, W) - S_{P_{X,0}}(u|0, W) \} du \right]}
  {\mathbb{E}_{P_{X,0}}[W_j^2]} \; .
$$

"$\Psi$" is re-used to highlight that this is an additive TEM-VIP. As in the
continuous outcome scenario, this parameter captures the correlation between the
conditional difference in the restricted mean survival time and the
$j^\text{th}$ confounder, re-normalized to be on the same scale as the outcome.
Put another way, this parameter identifies confounders driving the largest
difference in truncated survival times across treatment conditions.

As before, this full-data parameter is equal to the following observed-data
parameter under the assumptions of (1) no unmeasured confounding, (2) censoring
mechanism positivity, and (3) treatment assignment positivity:
$$
\Psi_j(P_0;t) = \frac{\mathbb{E}_{P_0}\left[W_j
  \int_0^t \{ S_0(u|1, W) - S_0(u|0, W) \} du \right]}
  {\mathbb{E}_{P_0}[W_j^2]} \; .
$$

As in the continuous and binary outcome scenarios, @boileau2023 derived two
estimators of this additive effect parameter: a one-step estimator and a TML
estimator. Both rely on the accurate estimation of three nuisance parameters:
the conditional event hazard function, the conditional censoring hazard
function, and the propensity score. If either the conditional event hazard
function or the conditional censoring hazard function and the propensity score
are consistently estimated, then these nonparametric estimators are consistent.
Further, if all of these nuisance parameters are converge to their true values
at a rate of $n^{-1/4}$, then these estimators are asymptotically normally
distributed. This permits Gaussian-based hypothesis testing through the use of
Wald-type confidence intervals. See @boileau2023 for more details.

We now apply these estimators to a random sample from the data-generating
process defined above, choosing as target parameter $\Psi(P_0; 8)$. LASSO
regressions are used to estimate the propensity scores and censoring hazard
functions, and, as demonstration of these methods' flexibility, estimate the
failure hazard function with XGBoost [@chen2016].

```{r tte-additive-tem-vip}
# simulate a random sample
sample_df <- tte_outcome_dgp(n_obs = 250)

# one-step estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers = c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "time",
  censoring = "censoring_indicator",
  time_cutoff = 8,
  outcome_type = "time-to-event",
  scale = "absolute",
  estimator = "onestep",
  prop_score_estimator = sl3::Lrnr_glmnet$new(),
  failure_hazard_estimator = sl3::Lrnr_xgboost$new(),
  censoring_hazard_estimator = sl3::Lrnr_glmnet$new()
)

# TML estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers = c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "time",
  censoring = "censoring_indicator",
  time_cutoff = 8,
  outcome_type = "time-to-event",
  scale = "absolute",
  estimator = "tmle",
  prop_score_estimator = sl3::Lrnr_glmnet$new(),
  failure_hazard_estimator = sl3::Lrnr_xgboost$new(),
  censoring_hazard_estimator = sl3::Lrnr_glmnet$new()
)
```

These results suggest every unit increase in `w_1` results in an increase of
$\approx 1.65$ --- according to the one-step estimator --- or $\approx 1.61$ ---
according to the TML estimator --- time units for the difference in mean
survival times, truncated at time eight.

### Relative TEM-VIP

We next consider a relative TEM-VIP for data with right-censored time-to-event
outcomes. Again assuming that the potential treatment effect modifiers are
centered and have variance bounded away from zero, we define this parameter as
follows for the $j^\text{th}$ confounder:
$$
\Gamma_j^F(P_{X,0};t) = \frac{\mathbb{E}_{P_{X,0}}\left[W_j
  (\log S_{P_{X,0}}(t|1, W) - \log S_{P_{X,0}}(t|0, W))\right]}
  {\mathbb{E}_{P_{X,0}}[W_j^2]} \; .
$$

For some pre-specified time point $t$, this TEM-VIP represents the standardized
correlation coefficient of $W_j$ and the log ratio of conditional survival times
under each treatment condition. As with "$\Psi$", we re-use "$\Gamma$" to
highlight that this is a relative effect parameter. Its observed-data
counterpart is given by
$$
\Gamma_j^F(P_0;t) = \frac{\mathbb{E}_{P_0}\left[W_j
  (\log S_0(t|1, W) - \log S_0(t|0, W))\right]}
  {\mathbb{E}_{P_0}[W_j^2]} \; ,
$$
again assuming that there is no unmeasured confounding and that there are no
positivity violations of the censoring or treatment assignment mechanisms.

@boileau2023 derived one-step and TML estimators of this parameter, again
requiring the estimation of three nuisance parameters: the propensity score and
the conditional failure and censoring event hazards. These one-step and TML
estimators are consistent when all nuisance parameters are estimated
consistently, and are asymptotically linear when these nuisance parameters
converge to their true values at a fast-enough rate [@boileau2023].

We apply the nonparametric estimators of @boileau2023 to a random sample
generated by `tte_outcome_dgp()`, taking as target of inference
$\Gamma_j(P_0; 5)$. The nuisance parameters are estimated using the same
strategy employed in prior absolute TEM-VIP example.

```{r tte-relative-tem-vip}
# simulate a random sample
sample_df <- tte_outcome_dgp(n_obs = 1000)

# one-step estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers = c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "time",
  censoring = "censoring_indicator",
  time_cutoff = 5,
  outcome_type = "time-to-event",
  scale = "relative",
  estimator = "onestep",
  prop_score_estimator = sl3::Lrnr_glmnet$new(),
  failure_hazard_estimator = sl3::Lrnr_xgboost$new(),
  censoring_hazard_estimator = sl3::Lrnr_glmnet$new(),
  cross_fit = TRUE
)

# TML estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3"),
  modifiers = c("w_1", "w_2", "w_3"),
  exposure = "a",
  outcome = "time",
  censoring = "censoring_indicator",
  time_cutoff = 5,
  outcome_type = "time-to-event",
  scale = "relative",
  estimator = "tmle",
  prop_score_estimator = sl3::Lrnr_glmnet$new(),
  failure_hazard_estimator = sl3::Lrnr_xgboost$new(),
  censoring_hazard_estimator = sl3::Lrnr_glmnet$new(),
  cross_fit = TRUE
)
```

Again, both the one-step and TML estimators successfully discern the treatment
effect modifier from the other potential confounders when using a false
discovery rate cutoff of $5\%$. These results indicate that a unit increase in
`w_1` results in an increase of the log ratio of conditional survivals equal to
either $\approx 3.32$ or $\approx 3.66$, depending on whether one uses the
one-step estimator or the TML estimator.


# Additional Features and Notes

## Working with Randomized Experiment Data

While `unihtee`'s estimators are applied to (simulated) observational data in
the previous section's examples, they are also suitable for data generated by
randomized experiments. The propensity scores used to randomize observations
should be saved data object passed to `unihtee()`, and the name of the variable
containing these scores should be indicated in the `propensity_score_values`
argument.

Providing the known propensity scores to the additive TEM-VIP estimators
produces some desirable results. In the continuous and binary outcome settings,
providing known treatment assignment probabilities guarantees that the
estimators are asymptotically linear, even when the expected conditional outcome
estimator is mispecdified. When the outcome is a right-censored time-to-event
variable, asymptotic normality of the estimators is guaranteed so long as the
conditional censoring hazard is estimated consistently at a rate of $n^{-1/4}$.
Additional details and discussions are provided in @boileau2022 and
@boileau2023.

## Cross-Fitting

Cross-fitting, popularized by @zheng2011cross and @cherno2017, uses
cross-validation to fit the nuisance parameters and estimate the target
parameter. Doing so relaxes the entropy constraints required for asymptotic
normality of the one-step and TML estimators implemented in the `unihtee`
package. This technique is also known to improve these estimators finite-sample
properties, such as reducing their false discovery rate (see, for example,
@hejazi2022).

Users may employ the cross-fitted version of any estimator implemented in
`unihtee` by setting `unihtee()`'s `cross_fit` argument to `TRUE`. Only K-fold
cross-validation is currently supported. The number of folds defaults to five,
though this can be modified using the `cross_fit_folds` argument. Examples are
provided in the binary outcome and time-to-event outcome relative TEM-VIP
sections of this vignette, as well as in the subsection on Super Learners that
follows.

## `sl3` Integration: Using Super Learners

`unihtee` uses the estimators implemented in the `sl3` [@coyle2021sl3-pkg] to
estimate nuisance parameters. While individual estimators may be used, as is
done in the examples above, `sl3`'s main contribution are facilities for
constructing Super Learners [@vdl2007]. Super Learners are ensemble algorithms,
meaning they are the convex combination of individual supervised learning
algorithms. These individual algorithms are combined on the basis of a
user-defined risk, and this combination is asymptotically optimal with respect
to said risk under minimal conditions about the data-generating process. A
complete review on Super Learners and `sl3` is outside the scope of this
vignette. We invite the interested reader to the tutorials available on the
`sl3` package's [website](https://tlverse.org/sl3/) and in Chapter 6 of the
[`tlverse` handbook](https://tlverse.org/tlverse-handbook/sl3.html)
[@tlverse-sl3].

Employing Super Learners to estimate nuisance parameters decreases the risk of
model-misspecification --- assuming, of course, that a diverse collection
individual learning algorithms is considered. In turn, there is an increased
likelihood that the resulting one-step and TML estimators are asymptotically
normal.

We revisit our continuous outcome example, this time using Super Learners to
estimate the propensity score and the conditional expected outcomes. Both Super
Learners are composed of penalized GLMs and Random Forests. The
propensity score's Super Learner uses the log loss to form the convex
combination of individual learning algorithms, while the conditional expected
outcomes' uses the squared error loss. Additionally, we employ 2-fold
cross-fitting.

```{r additive-tem-vip-w-sl}
# simulate a random sample
sample_df <- cont_outcome_dgp(n_obs = 500)

# define the Super Learner for the propensity score
sl_bin <- Lrnr_sl$new(
  learners = list(
    Lrnr_glmnet$new(family = "binomial", alpha = 0),
    Lrnr_glmnet$new(family = "binomial", alpha = 0),
    Lrnr_glmnet$new(family = "binomial", alpha = 0.5),
    Lrnr_ranger$new()
  ),
  metalearner = make_learner(
    Lrnr_solnp, metalearner_logistic_binomial,
    loss_squared_error
  )
)

# define the Super Learner for the conditional expected outcomes
sl_cont <- Lrnr_sl$new(
  learners = list(
    Lrnr_glmnet$new(
      family = "gaussian",
      formula = "~ a * w_1 + a * w_2 + a * w_3 + a * w_4 + a * w_5"
    ),
    Lrnr_glmnet$new(
      family = "gaussian",
      alpha = 0,
      formula = "~ a * w_1 + a * w_2 + a * w_3 + a * w_4 + a * w_5"
    ),
    Lrnr_glmnet$new(
      family = "gaussian",
      alpha = 0.5,
      formula = "~ a * w_1 + a * w_2 + a * w_3 + a * w_4 + a * w_5"
    ),
    Lrnr_ranger$new()
  ),
  metalearner = make_learner(
    Lrnr_solnp, metalearner_linear, sl3::loss_squared_error
  )
)

# one-step estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3", "w_4", "w_5"),
  modifiers =  c("w_1", "w_2", "w_3", "w_4", "w_5"),
  exposure = "a",
  outcome = "y",
  outcome_type = "continuous",
  scale = "absolute",
  estimator = "onestep",
  cond_outcome_estimator = sl_cont,
  prop_score_estimator = sl_bin,
  cross_fit = TRUE,
  cross_fit_folds = 2
)

# TML estimator
unihtee(
  data = sample_df,
  confounders = c("w_1", "w_2", "w_3", "w_4", "w_5"),
  modifiers =  c("w_1", "w_2", "w_3", "w_4", "w_5"),
  exposure = "a",
  outcome = "y",
  outcome_type = "continuous",
  scale = "absolute",
  estimator = "tmle",
  cond_outcome_estimator = sl_cont,
  prop_score_estimator = sl_bin,
  cross_fit = TRUE,
  cross_fit_folds = 2
)
```

Though this procedure is more robust, the resulting estimates and hypothesis
tests are largely similar to those of the previous continuous outcome, additive
TEM-VIP example.


# Session information

```{r session-information}
sessionInfo()
```
